---
title: "sta523 final project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

I. Introduction 

For this project, we want to analyze the average housing prices across different counties in the United States. Nowadays, more people begin to immigrate to the United States, including some of our friends. They move to different counties acrossing the country and brought their own houses. However, the price of their houses acting so differently acrossing each counties. 

Therefore, we are curious about the factors that will affect the price of the houses and we want to analyze the housing prices with different variables trying to find out the reasons behind that phenomenom. We also want to do some data visualizations to present a more clear picture to our audience about the distributions of housing price across different states and some time series housing price plots for some major counties. 


II. Project goals and the outlines 

1. To make an acurrate model that can predict the housing price with variables given.

2. Finding out the factors that may potentially affect the price of the houses. 

3. Data visualizations on housing price distributions and housing trends.

4. Built UI dashboard (Shiny) to visualize data and deliver data insight to audiences including plots & tables


III. Selecting the variables 

We use two methods to choose the variables that we want to scrape:

1. Focus group research

We conduct a survey to reach out our friends and our parents' friends who recently brought the houses in the United States or who planned to buy the houses in the United States. We ask them the factors that they thought are most important when they are planning to buy a house. We summarized these top factors into the potential variables pool one. 

2. Research Paper 

We read the research paper written by other people to learn which factors they may think are important to the price of houses. Some of the reseach paper from the economic department are very valuable to us because these authors conducted very comprehensive surveys and complicated models to conclude the major factors driving the price of the houses. So we summarized these factors into the potential variables pool two.

Surprisingly, we discovered that some factors appear in both potential variables pool one and pool two. So we decide to use these variables for our analyze. 


IV. Scraping the data and clean the data sets.

The final variables we want to include are population data, income data, hotel data, safety data, tax rate data, gasoline price data, college data, high school data, Traffic data, shopping mall(convinience) data and Hospital data. Since the time is limited otherwise there may be some few other data that we want to include. 

We find these data sources and scrape these data down using various methods. After that we clean the dataset to make sure it's tidy and can get prepared for the final modelling. We also simplified our codes to make sure it's clear and elegant. Finally, we combined all these data into one single dataframe which will be used for the final modelling and visualizations. 


V. Data checkings 

We do the final checks before we start to build the models. We draw some scatter plots and box plots for our varaibles, checking for any anomalies or NA values. Looking for potential outliers, etc. The final preparation stage before the modelling.


VI. Modelling 


VII. Visualizations 


VIII. User Interface (Shiny)


VIIII. Housing Recommendations 


Ten.  Conclusion. 


```{r, warning = F, message = F, output = F}
library(Quandl)
library(readr)
library(dplyr)
library(rvest)
library(stringr)
library(xlsx)
library(noncensus)
library(rvest)
library(tidyverse)
library(sqldf)
library(stringi)
library(readxl)
library(tools)
'%ni%' <- Negate('%in%')
```

## 1. base data

The base data, which is the housing price data across different counties is the base data we use for this project. 

This dataset contains the time series data about the housing price starting from 2013 to 2017. However, we only need the median housing price for each counties in Jan 2017 for the analyze. 

Because of that, the variables we scrape later also are data presented in 2017.

```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("pricepersqft.csv") %>%
  select(County,State, `17-Jan`) %>%
  group_by(County,State) %>%
  summarize(price = median(`17-Jan`))
```


## 2. Population data from wikipedia

This dataset contains the variables: Population, Per capita income, Median Household income, Median family income, and Numberof Households across each counties. The dataset is stored in Wikipedia. 

So we use the method we have learned from the class and the previous homework (read_html) to scrape this dataset down from the wikipedia. The data is stored in the wikitable so the html_nodes is "table.wikitable.sortable" and we only want the data in the first list so we use .[[1]]

At last we clean the data by filtering all empty values and replace all . with "". We cleaned the variable names and the variable classes and finally bind that tidy population dataset with our previous base dataset.

```{r message=FALSE, warning=FALSE}
county_data = "https://en.wikipedia.org/wiki/List_of_United_States_counties_by_per_capita_income" %>%
  read_html() %>%
  html_nodes("table.wikitable.sortable") %>%
  {.[[1]]} %>%
  html_table() %>%
  filter(State != "") %>%
  mutate_all(funs(str_replace_all(., "[\\$,]", ""))) %>%
  transmute(County = `County-equivalent`,
            State = state.abb[match(State, state.name)],
            Income_PerCapita = as.numeric(`Per capitaincome`),
            Median_Household_Income = as.numeric(Medianhouseholdincome),
            Median_Family_Income = as.numeric(Medianfamilyincome),
            Population = as.numeric(Population),
            Num_Household = as.numeric(`Number ofhouseholds`))
price_2017_2 = merge(price_2017,county_data,by = c('County','State'))
```


## 3. hilton data within the country

For this dataset, we want to get the numbers of Hilton in each counties. Actually the data we want to get is the hotel data, because we think that whether there's a large hotel inside the county can implies whether that's prosperous county. Often a prosperous county indicates high median housing price because more people are tend to live there. Therefore, we scrape the hilton data from the hilton website.

Just as we did in the homework, we first get all Urls for different hiltons acrossing the country and within these urls, we scrape the location of each hotels. However, we dont need the whole location because that's useless for us. We only need to know which counties it belongs. 

Therefore, we only need the zipcode within the location to match the hotel to the county. So we manipulate the location with several stringr functions such as str_replace, strsplit, substr to only get all ziocodes for all hiltons in the United States. 

Since We avoid the for loop we largely reduce the running time of scraping that data. But because we need to scrape so many websites within that single chunck so it may cause some time to run this chunk (but much shorter than we spent the time in our Denny's homework).  

```{r message=FALSE, warning=FALSE}
baseURL =  "https://www3.hilton.com"
links = read_html("https://www3.hilton.com/en/hotel-locations/us/index.html") %>%
    html_nodes(.,'ul.directory_locations_list li a') 
urls = html_attr(links, "href") %>% paste0(baseURL, .)

zips = sapply(urls, function(url){
     url %>%
         read_html() %>%
         html_nodes('ul.directory_locations_list li a') %>%
         {paste0(baseURL, html_attr(., "href"))} %>%
         sapply(function(locationURL){
             locationURL %>%
                 read_html() %>%
                 html_nodes('ul.directory_hotels_list li') %>%
                 html_text() %>%
                 str_replace("\t", "") %>%
                 str_replace("\r\n", "") %>%
                 {strsplit(., " ")[[1]]} %>%
                 {.[length(.)]} %>%
                 {strsplit(., "USA")[[1]][1]} %>%
                 substr(1, 5)
             })
 }) %>%
     unlist() %>%
     unname() %>%
    as.data.frame() 
colnames(zips) = "zips"
```

And the next step for us is to match the zipcodes to the counties. So we find a dataset online which contains all zipcodes, cities, counties, states data in the United States. This dataset is very important to us and we use a lot in our next few chuncks because it not only can match the zipcodes to the county but can also match the cities to counties.

So after we match the zipcodes with the corrsponding counties, there may be some counties which have more than 1 hilton in it. Therefore, we need to group by the counties and summarize the sum of hilton numbers within that counties. 

The final hilton data we merge into the base dataset is the number of hiltons within each counties. And we replace all NA with 0s. 

```{r message=FALSE, warning=FALSE}
zip_county_lookup = read_csv("ZIP-COUNTY-FIPS_2018-03.csv") %>%
    mutate(County = COUNTYNAME)

hilton_county = merge(zips,zip_county_lookup,by.x ='zips',by.y = 'ZIP') %>%
     mutate(COUNTYNAME = COUNTYNAME %>%
                str_locate_all(" ") %>%
                sapply(function(mat){mat[min(dim(mat), 2), 1] - 1}) %>%
                {str_sub(COUNTYNAME, 1, .)})

distinct_count = hilton_county%>%
  group_by(COUNTYNAME,STATE) %>%
  summarize(count = length(COUNTYNAME)) %>%
    mutate(hilton_count = count) %>% 
    dplyr::select(COUNTYNAME,STATE,hilton_count)
    
price_2017_3 = left_join(price_2017_2, distinct_count, by = c("County"="COUNTYNAME","State" = "STATE")) %>%
  mutate_all(funs(replace(., is.na(.), 0)))

```


## 4. clean the dataset 

```{r message=FALSE, warning=FALSE}
clean_county = function(df){
  df %>%
    mutate(County = str_replace(County, " County,*$| Borough,*$| Parish,*$| Municipality,*$| Municipio,*$| Island,*$", "") %>%
             str_replace(" [cC]ity,*$", " City"))
}

price_2017_4 = clean_county(price_2017_3) 
zip_county_lookup = clean_county(zip_county_lookup) %>%
  select(ZIP,STATE,County,CITY)
```

## 5. crime rate data

```{r message=FALSE, warning=FALSE}
crime_data = read_csv("crime_data_w_population_and_crime_rate.csv") %>%
  select(county_name, crime_rate_per_100000, IDNO, CPOPCRIM, CPOPARST, COVIND,
         MURDER, RAPE, ROBBERY, AGASSLT, BURGLRY, LARCENY, MVTHEFT, ARSON) %>%
  mutate(county_name = strsplit(county_name, " ")) %>%
  rename(County = county_name) %>%
  mutate(State = County %>% sapply(last),
         County = County %>% sapply(head, -1) %>% 
           sapply(function(v) {do.call(paste, as.list(v))})) %>%
  clean_county()

price_2017_5 = merge(price_2017_4,crime_data,by=c('County','State'))

crime_lost = price_2017_4[price_2017_4$County %ni% price_2017_5$County,]
w = which(crime_data$County == 'Larue')
crime_data$County[w] = crime_lost$County[1]
price_2017_5 = left_join(price_2017_4,crime_data,by=c('County','State'))
```

## 6. tax rate data

```{r message=FALSE, warning=FALSE}
setwd("TAXRATES_ZIP5")

tax_rate_data = list.files(pattern = "*.csv") %>%
  map(read.csv) %>%
  {do.call(bind_rows, .)} %>%
  left_join(zip_county_lookup,by=c("ZipCode"="ZIP")) %>%
  select(County, State, StateRate, EstimatedCombinedRate, EstimatedCountyRate,
         EstimatedCityRate, EstimatedSpecialRate, RiskLevel) %>%
  {.[!duplicated(.[,1:2]), ]}

Price_2017_6 = left_join(price_2017_5,tax_rate_data,by=c('County','State'))
```

## 7. gasoline price data

```{r message=FALSE, warning=FALSE}
get_gasoline = function(k){
    dat = trimws(html_text(html_nodes(web, paste0("td:nth-child(",k,")"))))
}

web = read_html("https://gasprices.aaa.com/state-gas-price-averages/")
gasoline_data = data.frame(State=get_gasoline(1), Regular=get_gasoline(2), MidGrade=get_gasoline(3), Premium=get_gasoline(4), Diesel=get_gasoline(5))

gasoline_data$State = state.abb[match(gasoline_data$State,state.name)]
price_2017_7 = left_join(Price_2017_6,gasoline_data,by='State')

```

## 8. number of colleges

```{r message=FALSE, warning=FALSE}
## reference: https://nces.ed.gov/ipeds/datacenter/InstitutionProfile.aspx

schools = read_html("IPEDS Data Center.html") %>%
    html_node("table.idc_gridview") %>%
    html_table() %>%
    select(-X1) %>% .[-1,]

colnames(schools) = c("school", "City", "State")

cities = zip_county_lookup %>% select(CITY, County, STATE) %>% unique() %>%
    rename(State = STATE, City = CITY)

school_data = left_join(schools,cities, by=c('City','State'))%>% 
    group_by(County, State) %>%
    summarise(college_num = n()) 

price_2017_8 = left_join(price_2017_7, school_data, by=c('County','State')) %>%
    mutate(college_num = ifelse(is.na(college_num), 0, college_num))
```

## 9. Traffic data 

```{r message=FALSE, warning=FALSE}
epidata = read_csv("EQIDATA_ALL_DOMAINS_2014MARCH11.CSV") %>% 
  select(county_name,state,hwyprop,ryprop,
         pct_pub_transport_log,fatal_rate_log,
         pct_pers_lt_pov,pct_unemp) %>% 
  rename(County=county_name,State=state)%>% 
  mutate(County=str_replace_all(County,"County","") %>% 
           str_trim(side = "both"))

price_2017_9 =left_join(price_2017_8, epidata,by=c('County','State'))%>%
  mutate_all(funs(replace(., is.na(.), 0)))

```

## 10. shopping mall data 

```{r message=FALSE, warning=FALSE}
read_wikitable_flat = function(state,n1,k,n4,n5,l4,l5){
    l1 = n1+1
    url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
    dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  html_table() %>% flatten()
  data=data.frame(
  name=c(dat[[1]] %>% as.character(.),dat[[n1]] %>% as.character(.),
         dat[[n1+k]] %>% as.character(.),dat[[n1+2*k]] %>% as.character(.),
         dat[[n4]] %>% as.character(.),dat[[n5]] %>% as.character(.)
  ),
  location=c(dat[[2]] %>% as.character(.),dat[[l1]] %>% as.character(.),
             dat[[l1+k]] %>% as.character(.),dat[[l1+2*k]] %>% as.character(.),
             dat[[l4]] %>% as.character(.),dat[[l5]] %>% as.character(.)
  ))
  return(data)
}

read_wikitable_nonflat = function(state){
  url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
  dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  html_table() 
  data = data_frame(
  name = dat[[1]][[1]] %>% as.character(.),
  location=dat[[1]][[2]] %>% as.character(.)
)
  return(data)
}

read_wikilist = function(state,n){
url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
page = read_html(url)
dat = data_frame(
  name=html_nodes(page,"li") %>% html_text() 
) %>% .[1:n,] %>%
  separate(name, c("name", "location"), "-",remove=TRUE)
}

read_texas = function(p){
 url = "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Texas"
 dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  .[[p]] %>% 
  html_table(fill=TRUE) 
dat = dat[,1:2]
colnames(dat)<-c("name","location")
return(dat)
}

page = read_html("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_the_United_States")

st_data = data_frame(
  name=html_nodes(page,"li") %>% html_text() 
) %>% .[59:879,] %>% 
  separate(name, c("name", "location1"), " . ",remove=TRUE)%>%
  separate(name, c("name", "location2"), "â€“",remove=TRUE, extra = "merge") %>% 
  mutate(location = coalesce(location1,location2)) %>%
  select(name, location)

shooping_mall_num = read_xlsx("shooping_mall_num.xlsx",1)
shopping = shooping_mall_num[rep(row.names(shooping_mall_num), shooping_mall_num$'Number of Malls'), 1] %>%
  as.data.frame() %>% .[1:821,]
st_data = st_data %>% mutate(STATE = shopping)

shopping_mall_data = bind_rows(
    al_data = read_wikitable_nonflat("Alabama") %>% mutate('STATE' = 'AL'),
    ca_data = read_wikilist("California",181) %>%
  separate(location, c("location", "state"), ",",remove=TRUE, extra = "merge") %>% 
  select(name, location) %>% 
  mutate('STATE' = 'CA'),
  md_data = read_wikilist("Maryland",34) %>% mutate('STATE' = 'MD'),
  mi_data = read_wikitable_flat("Michigan",8,7,29,36,30,37) %>% 
    separate(location, c("location", "latitude"), "4",remove=TRUE, extra = "merge") %>% select(name, location) %>% 
  mutate('STATE' = 'MI'),
  nj_data = read_wikitable_nonflat("New_Jersey") %>% mutate('STATE' = 'NJ'),
  or_data = read_wikitable_flat("Oregon",7,5,NA,NA,NA,NA) %>% 
  mutate('STATE' = 'OR'),
  pa_data = read_wikitable_nonflat("Pennsylvania") %>% mutate('STATE' = 'PA'),
  tx_data = rbind(read_texas(1),read_texas(2),read_texas(3),
                texas = read_wikilist("Texas",50) %>% .[15:50,] %>% separate(location, c("location", "other"), "[(]",remove=TRUE, extra = "merge") %>% 
  select(name, location)) %>% 
  mutate('STATE' = 'TX'),
  st_data)

shopping_mall_data_clean = shopping_mall_data %>%
  select(name, location)%>%
  separate(name, c("name", "other"), "[(]",remove=TRUE)%>%
  select(name, location)%>%
  separate(location, c("location", "other"), "[(]",remove=TRUE) %>%
  select(name, location)

shopping_mall_data = bind_cols(shopping_mall_data_clean,shopping_mall_data) %>% 
    select(name,location,STATE) %>% 
    rename(City = location, State = STATE)
```

```{r message=FALSE, warning=FALSE}
shopping_mall_data$City = tolower(shopping_mall_data$City) %>% str_trim(.,"left")
cities$City = tolower(cities$City)

shopping_mall_data_final = merge(shopping_mall_data,cities,by=c("City","State")) %>%
  select(name,City,State,County) %>% unique() %>%
  group_by(County,State) %>%
  summarise(num_shoppingmall = length(County)) 

price_2017_10 = left_join(price_2017_9,shopping_mall_data_final,by=c("County","State")) %>% 
    mutate(num_shoppingmall = ifelse(is.na(num_shoppingmall), 0, num_shoppingmall))
```


## 11. Public high school data

```{r message=FALSE, warning=FALSE}
public_school = read.csv("Public_high_school.csv", stringsAsFactors=FALSE) %>% 
    select(1, 2, 4) %>%
    rename(State = County.Name, County = County.Name..Public.School..2015.16, public_school_num = Total.Number.of.Public.Schools..Public.School..2015.16) %>%
    mutate(County = str_trim(str_replace(.$County, "County", "")),
           public_school_num = as.numeric(public_school_num),
           State = stri_sub(str_trim(State), -2, -1)) %>% 
    na.omit() %>%
    group_by(County, State) %>% 
    summarise(public_school_count = sum(public_school_num, na.rm = TRUE))

price_2017_11 = left_join(price_2017_10, public_school, by=c('County','State')) %>%
    mutate(public_school_count = ifelse(is.na(public_school_count), 0, public_school_count))
```

## 12. Private high school data

```{r message=FALSE, warning=FALSE}
private_school = read.csv("Private_high_school.csv", stringsAsFactors=FALSE) %>% 
    select(2, 5) %>% 
    rename(State = State.Name..Private.School..Latest.available.year, County = County.Name..Private.School..2015.16) %>% 
    mutate(State = state.abb[match(toTitleCase(tolower(State)),state.name)],
           County = toTitleCase(tolower(County))) %>%
    group_by(County, State) %>% 
    summarise(private_school_count = n()) 

price_2017_12 = left_join(price_2017_11, private_school, by=c('County','State')) %>%
    mutate(private_school_count = ifelse(is.na(private_school_count), 0, private_school_count))
```


## 13. Hospital data

```{r message=FALSE, warning=FALSE}
hospital = private_school = read.csv("HospInfo.csv", stringsAsFactors=FALSE) %>%
    select(Hospital.Name, County.Name, State,Emergency.Services, Hospital.overall.rating ) %>% rename(Name = Hospital.Name, County = County.Name, Emergency= Emergency.Services, rating = Hospital.overall.rating) %>%
    mutate(County = toTitleCase(tolower(County)),
           Emergency = as.numeric(Emergency),
           rating = as.numeric(rating)) %>%
    group_by(County, State) %>%
    summarise(num_hospitals = length(County),
              num_emergency = sum(Emergency),
              hospital_rating = mean(rating,na.rm = TRUE)) %>%
    mutate(hospital_rating = ifelse(is.nan(hospital_rating), 0, hospital_rating))

price_2017_13 = left_join(price_2017_12, hospital, by=c('County','State')) %>%
    mutate(num_hospitals = ifelse(is.na(num_hospitals), 0, num_hospitals),
           num_emergency = ifelse(is.na(num_emergency), 0, num_emergency),
           hospital_rating = ifelse(is.na(hospital_rating), 0, hospital_rating))
```


```{r}
county_data = price_2017_13
save(county_data, file = "county_data.RData")
```


