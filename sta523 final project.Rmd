---
title: "sta523 final project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

I. Introduction 

For this project, we want to analyze the average housing prices across different counties in the United States. Nowadays, more people begin to immigrate to the United States, including some of our friends. They move to different counties acrossing the country and brought their own houses. However, the price of their houses acting so differently acrossing each counties. 

Therefore, we are curious about the factors that will affect the price of the houses and we want to analyze the housing prices with different variables trying to find out the reasons behind that phenomenom. We also want to do some data visualizations to present a more clear picture to our audience about the distributions of housing price across different states and some time series housing price plots for some major counties. 


II. Project goals and the outlines 

1. To make an acurrate model that can predict the housing price with variables given.

2. Finding out the factors that may potentially affect the price of the houses. 

3. Data visualizations on housing price distributions and housing trends.

4. Built UI dashboard (Shiny) to visualize data and deliver data insight to audiences including plots & tables


III. Selecting the variables 

We use two methods to choose the variables that we want to scrape:

1. Focus group research

We conduct a survey to reach out our friends and our parents' friends who recently brought the houses in the United States or who planned to buy the houses in the United States. We ask them the factors that they thought are most important when they are planning to buy a house. We summarized these top factors into the potential variables pool one. 

2. Research Paper 

We read the research paper written by other people to learn which factors they may think are important to the price of houses. Some of the reseach paper from the economic department are very valuable to us because these authors conducted very comprehensive surveys and complicated models to conclude the major factors driving the price of the houses. So we summarized these factors into the potential variables pool two.

Surprisingly, we discovered that some factors appear in both potential variables pool one and pool two. So we decide to use these variables for our analyze. 


IV. Scraping the data and clean the data sets.

The final variables we want to include are population data, income data, hotel data, safety data, tax rate data, gasoline price data, college data, high school data, Traffic data, shopping mall(convinience) data and Hospital data. Since the time is limited otherwise there may be some few other data that we want to include. 

We find these data sources and scrape these data down using various methods. After that we clean the dataset to make sure it's tidy and can get prepared for the final modelling. We also simplified our codes to make sure it's clear and elegant. Finally, we combined all these data into one single dataframe which will be used for the final modelling and visualizations. 


V. Data checkings (EDA)

We do the final checks before we start to build the models. We draw some scatter plots and box plots for our varaibles, checking for any anomalies or NA values. Looking for potential outliers, etc. The final preparation stage before the modelling.

The final checkings includes: 1. There is no missing values, 2. There is no serious outliers, 3. Any transformation of varaibles needed, 4. Any anomalies of dataset, 
5. Distributions are not weird


VI. Modelling 

This section we try to build the models for our data. Our models have two ultimate goals:

1. Accurately predict the value of the housing by the data given

2. Find out the important factors that will affect the price of the house. 

We will use RMSE and R^2 to be the main index to test the model accuracy. And we will test Linear models, Random Forest, and deep learning neural network to see which one performs the best.

Before we start to build the models, We rescale the mean and the standard deviation for the variables (needed for some machine learning models) and we randomly split the dataset into two parts: train set and the test set. 

For the first goal, we decide to use the fully connected neural networks + early stopping + bagging method to predict for the housing price. It's pretty accurate and more precise than the other models. Yielding relatively low RMSE on the test set(0.4626) and relatively higher R^2 on the test set(0.59).


For the Second goal, we got the important factors that will affect the housing price are (in the order of importance):

1. Safety rate

2. Income level

3. CPI (spending level)

4. Traffic situation (transportation)

5. Tax rate

6. Education (colleges and high schools)

7. Economic situation (Unemployment rate and poverty rate)

8. Number of shopping malls


VII. Visualizations 


VIII. User Interface (Shiny)


VIIII. Housing Recommendations 


Ten.  Conclusion. 


```{r, warning = F, message = F, output = F}
library(Quandl)
library(readr)
library(dplyr)
library(rvest)
library(stringr)
library(xlsx)
library(noncensus)
library(rvest)
library(tidyverse)
library(sqldf)
library(stringi)
library(readxl)
library(tools)
library(gridExtra)
library(BAS)
library(randomForest)
library(neuralnet)
suppressMessages(library(shiny))
suppressMessages(library(shinythemes))
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
suppressMessages(library(urbnmapr))
'%ni%' <- Negate('%in%')
library(fmsb)
library(tidyverse)
```

## 1. base data

The base data, which is the housing price data across different counties is the base data we use for this project. 

This dataset contains the time series data about the housing price starting from 2013 to 2017. However, we only need the median housing price for each counties in Jan 2017 for the analyze. 

Because of that, the variables we scrape later also are data presented in 2017.

```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("pricepersqft.csv") %>%
  select(County,State, `17-Jan`) %>%
  group_by(County,State) %>%
  summarize(price = median(`17-Jan`))
```


## 2. Population data from wikipedia

This dataset contains the variables: Population, Per capita income, Median Household income, Median family income, and Numberof Households across each counties. The dataset is stored in Wikipedia. 

So we use the method we have learned from the class and the previous homework (read_html) to scrape this dataset down from the wikipedia. The data is stored in the wikitable so the html_nodes is "table.wikitable.sortable" and we only want the data in the first list so we use .[[1]]

At last we clean the data by filtering all empty values and replace all . with "". We cleaned the variable names and the variable classes and finally bind that tidy population dataset with our previous base dataset.

```{r message=FALSE, warning=FALSE}
price_2017 = "https://en.wikipedia.org/wiki/List_of_United_States_counties_by_per_capita_income" %>%
  read_html() %>%
  html_nodes("table.wikitable.sortable") %>%
  {.[[1]]} %>%
  html_table() %>%
  filter(State != "") %>%
  mutate_all(funs(str_replace_all(., "[\\$,]", ""))) %>%
  transmute(County = `County-equivalent`,
            State = state.abb[match(State, state.name)],
            Income_PerCapita = as.numeric(`Per capitaincome`),
            Median_Household_Income = as.numeric(Medianhouseholdincome),
            Median_Family_Income = as.numeric(Medianfamilyincome),
            Population = as.numeric(Population),
            Num_Household = as.numeric(`Number ofhouseholds`)) %>%
    merge(price_2017,.,by = c('County','State'))
```


## 3. hilton data within the country

For this dataset, we want to get the numbers of Hilton in each counties. Actually the data we want to get is the hotel data, because we think that whether there's a large hotel inside the county can implies whether that's prosperous county. Often a prosperous county indicates high median housing price because more people are tend to live there. Therefore, we scrape the hilton data from the hilton website.

Just as we did in the homework, we first get all Urls for different hiltons acrossing the country and within these urls, we scrape the location of each hotels. However, we dont need the whole location because that's useless for us. We only need to know which counties it belongs. 

Therefore, we only need the zipcode within the location to match the hotel to the county. So we manipulate the location with several stringr functions such as str_replace, strsplit, substr to only get all ziocodes for all hiltons in the United States. 

Since We avoid the for loop we largely reduce the running time of scraping that data. But because we need to scrape so many websites within that single chunck so it may cause some time to run this chunk (but much shorter than we spent the time in our Denny's homework).  

And the next step for us is to match the zipcodes to the counties. So we find a dataset online which contains all zipcodes, cities, counties, states data in the United States. This dataset is very important to us and we use a lot in our next few chuncks because it not only can match the zipcodes to the county but can also match the cities to counties.

So after we match the zipcodes with the corrsponding counties, there may be some counties which have more than 1 hilton in it. Therefore, we need to group by the counties and summarize the sum of hilton numbers within that counties. 

The final hilton data we merge into the base dataset is the number of hiltons within each counties. And we replace all NA with 0s. 

```{r message=FALSE, warning=FALSE}
zip_county_lookup = read_csv("ZIP-COUNTY-FIPS_2018-03.csv") %>%
    mutate(County = COUNTYNAME)

urls = read_html("https://www3.hilton.com/en/hotel-locations/us/index.html") %>%
    html_nodes(.,'ul.directory_locations_list li a') %>% 
    html_attr(., "href") %>% paste0("https://www3.hilton.com", .)

price_2017 = sapply(urls, function(url){
     url %>%
         read_html() %>%
         html_nodes('ul.directory_locations_list li a') %>%
         {paste0("https://www3.hilton.com", html_attr(., "href"))} %>%
         sapply(function(locationURL){
             locationURL %>%
                 read_html() %>%
                 html_nodes('ul.directory_hotels_list li') %>%
                 html_text() %>%
                 str_replace("\t", "") %>%
                 str_replace("\r\n", "") %>%
                 {strsplit(., " ")[[1]]} %>%
                 {.[length(.)]} %>%
                 {strsplit(., "USA")[[1]][1]} %>%
                 substr(1, 5)
             })
 }) %>%
     unlist() %>%
     unname() %>%
    as.data.frame() %>%
    {colnames(.)[1] = "zips"; .} %>%
    merge(.,zip_county_lookup,by.x ='zips',by.y = 'ZIP') %>%
     mutate(COUNTYNAME = COUNTYNAME %>%
                str_locate_all(" ") %>%
                sapply(function(mat){mat[min(dim(mat), 2), 1] - 1}) %>%
                {str_sub(COUNTYNAME, 1, .)}) %>%
  group_by(COUNTYNAME,STATE) %>%
  summarize(count = length(COUNTYNAME)) %>%
    mutate(hilton_count = count) %>% 
    dplyr::select(COUNTYNAME,STATE,hilton_count) %>%
    left_join(price_2017, ., by = c("County"="COUNTYNAME","State" = "STATE")) %>%
  mutate_all(funs(replace(., is.na(.), 0)))
```


## 4. clean the dataset 

In order to merge more easily and efficiently for the later chuncks, we want to modify the County column in both base datasets and the zip_county_lookup(the matching dataset.)

For both datasets, we remove County, Borough, Parish, Munipality, Municipio, Island and We replace all "city" by "City". Now the two datasets' County column are identical and easy to merge in the later tasks. 

```{r message=FALSE, warning=FALSE}
clean_county = function(df){
  df %>%
    mutate(County = str_replace(County, " County,*$| Borough,*$| Parish,*$| Municipality,*$| Municipio,*$| Island,*$", "") %>%
             str_replace(" [cC]ity,*$", " City"))
}

price_2017 = clean_county(price_2017) 
zip_county_lookup = clean_county(zip_county_lookup) %>%
  select(ZIP,STATE,County,CITY)
```

## 5. crime rate data

For this chunck we want to get the crime rate data for each county. Cause safety is one of the most important factor to consider when people are buying houses to live. No one wants to live in a very dangerous place. 

Therefore, we find out the crime dataset from the online sources and we acquire the safety related variables such as crime percent, murder rate, rate rate, robbery rate, theft rate etc. 

We clean the crime dataframe and merge it into the base dataset. Since in the crimerate dataset, the county and state are stick together: aaa county, xx state. Therefore, we use strsplit and mutate to divide the county and state and we use the clean_county function we defined above to clean the crime_data dataframe.After that, we merge it with the base dataset.

However, one observation missing after the merging and after checking for that variables we discover that in crime dataset that obversvation names "Larue" but it should named "LaRue" in our base dataset. So we modified that observation and merge agian. 

```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("crime_data_w_population_and_crime_rate.csv") %>%
  select(county_name, crime_rate_per_100000, IDNO, CPOPCRIM, CPOPARST, COVIND,
         MURDER, RAPE, ROBBERY, AGASSLT, BURGLRY, LARCENY, MVTHEFT, ARSON) %>%
  mutate(county_name = strsplit(county_name, " ")) %>%
  rename(County = county_name) %>%
  mutate(State = County %>% sapply(last),
         County = County %>% sapply(head, -1) %>% 
           sapply(function(v) {do.call(paste, as.list(v))})) %>%
  clean_county() %>%
    left_join(price_2017,.,by=c('County','State'))
```

## 6. tax rate data

We decide that the tax rate is also a very important indicator to the housing price because people may want to live in the places with lower tax rates. So we get the online data sources for the tax rates for all zips in the United States.

However, that data is a zipfile and after unzip it, it becomes a folder. So we first read all csv data into one single dataframe and we join it with the counties by using the zip_county_lookup. We choose the appropriate varialbes that can illustrate the tax rate and use {.[!duplicated(.[,1:2]), ]} as to filter out only unique Counties. 

Finally, we merge that tax rate dataset with our base dataset. 

```{r message=FALSE, warning=FALSE}
setwd("TAXRATES_ZIP5")

price_2017 = list.files(pattern = "*.csv") %>%
  map(read.csv) %>%
  {do.call(bind_rows, .)} %>%
  left_join(zip_county_lookup,by=c("ZipCode"="ZIP")) %>%
  select(County, State, StateRate, EstimatedCombinedRate, EstimatedCountyRate,
         EstimatedCityRate, EstimatedSpecialRate, RiskLevel) %>%
  {.[!duplicated(.[,1:2]), ]} %>%
    left_join(price_2017,.,by=c('County','State')) 
```

## 7. gasoline price data

We think that gasoline price can be a very interesting indicator that can show the price of the housing. This was the result illustrated by one of the paper we read. Gasoline price can somehow reflect an area's CPI and thus making an impact on the housing price.

So we find a website containing the gasoline price for each states in the United States (We tried very hard but we cannot find the average gasoline price by County unless we have to pay 50+ dollar for it). We write the function get_gasoline to scrape the data from this website and to directly construct a dataframe by using the data scraped by this function.

However, in the website the price is stored as for example: $3.566. This is a factor level. Therefore, we use the gsub to remove all dollar signs and change all the classes to the numeric. 

Finally, we merge the gasoline dataset with our base dataset. 

```{r message=FALSE, warning=FALSE}
get_gasoline = function(k){
    web = read_html("https://gasprices.aaa.com/state-gas-price-averages/")
    dat = trimws(html_text(html_nodes(web, paste0("td:nth-child(",k,")"))))
    }

price_2017 = data.frame(State=get_gasoline(1), Regular=get_gasoline(2), MidGrade=get_gasoline(3), Premium=get_gasoline(4), Diesel=get_gasoline(5)) %>% lapply(., gsub, pattern="\\$", replacement="") %>% 
    as.data.frame() %>% 
    mutate(Regular = as.numeric(as.character(Regular)),
           MidGrade = as.numeric(as.character(MidGrade)),
           Premium = as.numeric(as.character(Premium)),
           Diesel = as.numeric(as.character(Diesel))) %>%
    mutate(State = state.abb[match(State,state.name)]) %>%
    left_join(price_2017,.,by='State')
```

## 8. number of colleges

We think that the number of college is also a very important indicator to the price of the houses because college will attract many students coming from different places in the world to that county and these students need place to live. 

So we scrape the data from the IPEDS Data Center to get all universities/colleges in the country and their corresponding cities. Then we use the data zip_county_lookup again to get the reference from cities to counties. We dont need the first column (school IDS) and we dont need the first row (title and columnnames). 

After matching these schools with their corresponding counties we summarize the number of colleges in each counties and finally we merge that college dataset to our base dataset. We replace NA with 0s. 

```{r message=FALSE, warning=FALSE}
## reference: https://nces.ed.gov/ipeds/datacenter/InstitutionProfile.aspx

cities = zip_county_lookup %>% select(CITY, County, STATE) %>% unique() %>%
    rename(State = STATE, City = CITY)

price_2017 = read_html("IPEDS Data Center.html") %>%
    html_node("table.idc_gridview") %>%
    html_table() %>%
    select(-X1) %>% .[-1,] %>%
    rename(school = X2, City = X3, State = X4) %>%
    left_join(.,cities, by=c('City','State'))%>% 
    group_by(County, State) %>%
    summarise(college_num = n()) %>%
    left_join(price_2017, ., by=c('County','State')) %>%
    mutate(college_num = ifelse(is.na(college_num), 0, college_num))
```


## 9. Traffic data 

Both the paper and the research group shows that the Traffic situation really affect the housing prices. No ones want to live in a place where they have to spent hours trafficing on the road. So we find a traffic dataset from the online sorces and after some manipulations on the dataset, we merge this data to our base dataset(dont talk too much here because it's very similar to what we did above).


```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("EQIDATA_ALL_DOMAINS_2014MARCH11.CSV") %>% 
  select(county_name,state,hwyprop,ryprop,
         pct_pub_transport_log,fatal_rate_log,
         pct_pers_lt_pov,pct_unemp) %>% 
  rename(County=county_name,State=state)%>% 
  mutate(County=str_replace_all(County,"County","") %>% 
           str_trim(side = "both")) %>%
    left_join(price_2017, .,by=c('County','State'))%>%
  mutate_all(funs(replace(., is.na(.), 0)))
```

## 10. shopping mall data 

Number of Shopping mall is an important indicator to the housing price because of 

1: if a county has one or more shopping mall it can at least show it's a prosperous county

2: Shopping mall itself can attract people come/ travel to that county

3. More shopping mall implies more people live nearby, thus higher housing price.

Therefore, we scrape the shopping mall data from the online sources.


However, that one is a bit tricky. Because for different states, shopping mall are stored in different websites, and the storing patterns are not the same. Some are stored in the table form, some are stored in the list form. And for both table form and list form, different webpage has different stroing methods. So basically we are scraping many different websites in this part and these websites are not identical.

For data stored in wikitable forms, it has two categories: needed to be flatten and dont needed to be flatten. So I write two different functions o scrape these data. 

For the table which needs to be flatten:

the input are state name (used for the url), n1, the second data list we needed (the first data list is 1), k, the interval between each data list we needed, and n4,n5,l4,l5 are just the last two names and last two locations of shopping mall data list we needed. We need these n4,n5,l4,l5 because for some states they only have 4 data lists we needed for both name and location, so we can use NA for these 4 inputs. 

After we read and flatten the wikitable, we got a list that inside this large list, some of the list contains the name and location we want and some contains the information we dont want. So we have to acquire the corresponding data we want and to make it into the dataframe with two columns: name and location. And we return that dataframe. 


For the table dont need to be flatten:

That's much easier. we simply read that wikitable and the whole list did not contain the data we dont want. Simply all names are stored in the list 1 and all locations are stored in the list 2. So we return that dataset with two columns: name and location. The input is only state using for the url.


For the lists:

We use "html_nodes(page,"li") %>% html_text()" to scrape the corresponding data we needed from the list in wikipedia. 

We also need an input n because in the lists, after the shopping mall name and location, there are also other information stored in that list that we dont want. Therefore, because we already know the number of shopping mall within each state, we have to input that n (the number of shopping mall in that state = the number of useful rows in the list) so we are not getting any annoying data. 

This is not hardcoding because there's no other way to do it more efficiently and we have tried our best to simplify the code here.


Therefore, we scrape the data for different states directly using these three functions but for texas, it's different. Because in the texas shopping mall website, data is stored in different wikitables and lists. So i write a function read_taxas and the input p is simply the ID of wikitable stored in that website.


After scraping all these dataset down we have to clean the location data because the original dataset are very messy. We use seperate the select to remove all unuseful (), latitute, longitude, state information inside the location.

For each state's data We scrape, we mutate a new column state and generate the corresponding state to it in order for the group by below.

Finally, we bind all these data into the dataframe shopping mall data. 

In that final shopping mall data we still have some problems: all letters are in the uppercase and there's some head blanks in some of the observations.

So we did some data manipulations and merge it with the cities data we constructed before (to match the city to the counties).

After matching to the counties, we summarize the number of shopping malls in each counties and merge it with the base dataset. Replacing NA with 0s. 

```{r message=FALSE, warning=FALSE}
read_wikitable_flat = function(state,n1,k,n4,n5,l4,l5){
    l1 = n1+1
    url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
    dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  html_table() %>% flatten()
  data=data.frame(
  name=c(dat[[1]] %>% as.character(.),dat[[n1]] %>% as.character(.),
         dat[[n1+k]] %>% as.character(.),dat[[n1+2*k]] %>% as.character(.),
         dat[[n4]] %>% as.character(.),dat[[n5]] %>% as.character(.)
  ),
  location=c(dat[[2]] %>% as.character(.),dat[[l1]] %>% as.character(.),
             dat[[l1+k]] %>% as.character(.),dat[[l1+2*k]] %>% as.character(.),
             dat[[l4]] %>% as.character(.),dat[[l5]] %>% as.character(.)
  ))
  return(data)
}

read_wikitable_nonflat = function(state){
  url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
  dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  html_table() 
  data = data_frame(
  name = dat[[1]][[1]] %>% as.character(.),
  location=dat[[1]][[2]] %>% as.character(.)
)
  return(data)
}

read_wikilist = function(state,n){
url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
page = read_html(url)
dat = data_frame(
  name=html_nodes(page,"li") %>% html_text() 
) %>% .[1:n,] %>%
  separate(name, c("name", "location"), "-",remove=TRUE)
}

read_texas = function(p){
 url = "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Texas"
 dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  .[[p]] %>% 
  html_table(fill=TRUE) 
dat = dat[,1:2]
colnames(dat)<-c("name","location")
return(dat)
}

shooping_mall_num = read_xlsx("shooping_mall_num.xlsx",1)
shopping = shooping_mall_num[rep(row.names(shooping_mall_num), shooping_mall_num$'Number of Malls'), 1] %>%
  as.data.frame() %>% .[1:821,]

cities$City = tolower(cities$City)

page = read_html("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_the_United_States")

price_2017 = bind_rows(
    al_data = read_wikitable_nonflat("Alabama") %>% mutate('STATE' = 'AL'),
    ca_data = read_wikilist("California",181) %>%
  separate(location, c("location", "state"), ",",remove=TRUE, extra = "merge") %>% 
  select(name, location) %>% 
  mutate('STATE' = 'CA'),
  md_data = read_wikilist("Maryland",34) %>% mutate('STATE' = 'MD'),
  mi_data = read_wikitable_flat("Michigan",8,7,29,36,30,37) %>% 
    separate(location, c("location", "latitude"), "4",remove=TRUE, extra = "merge") %>% select(name, location) %>% 
  mutate('STATE' = 'MI'),
  nj_data = read_wikitable_nonflat("New_Jersey") %>% mutate('STATE' = 'NJ'),
  or_data = read_wikitable_flat("Oregon",7,5,NA,NA,NA,NA) %>% 
  mutate('STATE' = 'OR'),
  pa_data = read_wikitable_nonflat("Pennsylvania") %>% mutate('STATE' = 'PA'),
  tx_data = rbind(read_texas(1),read_texas(2),read_texas(3),
                texas = read_wikilist("Texas",50) %>% .[15:50,] %>% separate(location, c("location", "other"), "[(]",remove=TRUE, extra = "merge") %>% 
  select(name, location)) %>% 
  mutate('STATE' = 'TX'),
  st_data = data_frame(
  name=html_nodes(page,"li") %>% html_text() 
) %>% .[59:879,] %>% 
  separate(name, c("name", "location1"), " . ",remove=TRUE)%>%
  separate(name, c("name", "location2"), "–",remove=TRUE, extra = "merge") %>% 
  mutate(location = coalesce(location1,location2)) %>%
  select(name, location) %>% mutate(STATE = shopping)) %>%
  separate(name, c("name", "other"), "[(]",remove=TRUE) %>%
  select(name, location, STATE)%>%
  separate(location, c("location", "other"), "[(]",remove=TRUE) %>%
  select(name, location, STATE) %>%
    rename(City = location, State = STATE) %>%
    mutate(City = tolower(City) %>% str_trim(.,"left")) %>%
  merge(.,cities,by=c("City","State")) %>%
  select(name,City,State,County) %>% unique() %>%
  group_by(County,State) %>%
  summarise(num_shoppingmall = length(County)) %>%
  left_join(price_2017,.,by=c("County","State")) %>%
    mutate(num_shoppingmall = ifelse(is.na(num_shoppingmall), 0, num_shoppingmall))
    
    

```

## 11. Public high school data

As the same reason as the colleges, high school also attract people coming from different places around the world. So we find the high school data (both public and private) from the online data sources and merge it into our base dataset.

We dont want to illustrate too much here because basically we are using the very similar data cleaning as we are using above. 

```{r message=FALSE, warning=FALSE}
price_2017 = read.csv("Public_high_school.csv", stringsAsFactors=FALSE) %>% 
    select(1, 2, 4) %>%
    rename(State = County.Name, County = County.Name..Public.School..2015.16, public_school_num = Total.Number.of.Public.Schools..Public.School..2015.16) %>%
    mutate(County = str_trim(str_replace(.$County, "County", "")),
           public_school_num = as.numeric(public_school_num),
           State = stri_sub(str_trim(State), -2, -1)) %>% 
    na.omit() %>%
    group_by(County, State) %>% 
    summarise(public_school_count = sum(public_school_num, na.rm = TRUE)) %>%
    left_join(price_2017, ., by=c('County','State')) %>%
    mutate(public_school_count = ifelse(is.na(public_school_count), 0, public_school_count))
```

## 12. Private high school data

The same reason as in the part 11.

For here we use state.abb[match(toTitleCase(tolower(State)),state.name)] to transform the state name in the abbreviations by using the function state.abb. toTitleCase is to only transform the first letter into capital so it can match to the state.name. 

```{r message=FALSE, warning=FALSE}
price_2017 = read.csv("Private_high_school.csv", stringsAsFactors=FALSE) %>% 
    select(2, 5) %>% 
    rename(State = State.Name..Private.School..Latest.available.year, County = County.Name..Private.School..2015.16) %>% 
    mutate(State = state.abb[match(toTitleCase(tolower(State)),state.name)],
           County = toTitleCase(tolower(County))) %>%
    group_by(County, State) %>% 
    summarise(private_school_count = n()) %>%
    left_join(price_2017, ., by=c('County','State')) %>%
    mutate(private_school_count = ifelse(is.na(private_school_count), 0, private_school_count))

```


## 13. Hospital data

Hospital is a very important indicator for the housing price because everyone will sick in some of their life and they want good hospitals to treat.Therefore, if a county has many good hospitals it can attract people to live here.

Therefore, we find a dataset containing all hospitals in the country and also their ratings, emergency service, etc. 

We summarize number of hospitals, number of emergency hospitals and average hospital ratings for each country and merge them to the base dataset. Replacing NA with 0 and if a county doesnt have a hospital will replace 0 with its rating. 


```{r message=FALSE, warning=FALSE}
price_2017 = read.csv("HospInfo.csv", stringsAsFactors=FALSE) %>%
    select(Hospital.Name, County.Name, State,Emergency.Services, Hospital.overall.rating ) %>% rename(Name = Hospital.Name, County = County.Name, Emergency= Emergency.Services, rating = Hospital.overall.rating) %>%
    mutate(County = toTitleCase(tolower(County)),
           Emergency = as.numeric(Emergency),
           rating = as.numeric(rating)) %>%
    group_by(County, State) %>%
    summarise(num_hospitals = length(County),
              num_emergency = sum(Emergency),
              hospital_rating = mean(rating,na.rm = TRUE)) %>%
    mutate(hospital_rating = ifelse(is.nan(hospital_rating), 0, hospital_rating)) %>% left_join(price_2017, ., by=c('County','State')) %>%
    mutate(num_hospitals = ifelse(is.na(num_hospitals), 0, num_hospitals),
           num_emergency = ifelse(is.na(num_emergency), 0, num_emergency),
           hospital_rating = ifelse(is.na(hospital_rating), 0, hospital_rating))
```


```{r}
county_data = price_2017
save(county_data, file = "county_data.RData")
```

```{r}
load("county_data.RData")
```

```{r}
county_data = county_data %>%
    mutate(hilton_count = as.factor(hilton_count),
           num_shoppingmall = as.factor(num_shoppingmall),
           num_hospitals = as.factor(num_hospitals),
           num_emergency = as.factor(num_emergency))
```


Model EDA

Before we start to build the models, we want to do the final round to check the dataset to make sure:

1. There is no missing values

2. There is no serious outliers

3. Any transformation of varaibles needed

4. Any anomalies of dataset 

5. Distributions are not weird

To do this, we first check the missing values and we conclude that there's no missing values in our current dataset. 

And then we take a carefully look on each variables to make sure there's no any anomalies of any variables and the distributions are not weird.

At last, we make a scatter plot/box plot for each varaibles to check if any variables need the transformation and if there's any serious outiers.


The function create_plot create the analysis plot for each varaibles: if it's a factor, create a box plot. If it's a numeric value, create a scatter plot. And Subplot combine 9 plots into one so that the audience will not see long pages of only EDA plots. 

At last, we use a walk to show all the box plots/scatter plots for our analysis. 


```{r}
which(is.na(county_data))

create_plot = function(input, name){
    if(is.factor(input)){
        p = data_frame(
            price = county_data$price,
            input = input
        ) %>% 
            ggplot(aes(x = input, y = price)) + 
            geom_boxplot() + 
            labs(x = name)
    }else{
        p = data_frame(
            price = county_data$price,
            input = input
        ) %>% 
            ggplot(aes(x = input, y = price)) + 
            geom_point() + 
            labs(x = name)
    }
    return(p)
}

subplots = function(df, cols){
    eda_plots = map(cols, function(i){create_plot(df[, i], names(df)[i])})
    grid.arrange(grobs = eda_plots, nrow = 3, ncol = 3)
}


walk(1:(ncol(county_data %>% select(- County, - State, - price) ) %/% 9 + 1), function(group_num){
    col_index = (group_num * 9  - 8):min(group_num * 9, ncol(df))
    subplots(county_data %>% select(- County, - State, - price), col_index)
})
```

After looking at the scatter plots, we decide to log transform some of the variables to make it more significant (more like a linear relationship to the y variable). 

In addition, we discover that hilton count, num_shopping mall, num_hospitals and num_emergency have linear characteristics (as their values increase, the average price of house increases) and at the same time they have so many levels. Therefore, we decide to not use them as factor level but as numeric. 

After that, our final cleaning is done and we are ready to the final modellings. 


```{r}
county_data = county_data %>%
    mutate(
     Population = log(Population),
     Num_Household = log(Num_Household),
     hilton_count = as.numeric(as.character(hilton_count)),
     num_shoppingmall = as.numeric(as.character(num_shoppingmall)),
     num_hospitals = as.numeric(as.character(num_hospitals)),
     num_emergency = as.numeric(as.character(num_emergency))
    )
```



Modelling

This section we try to build the models for our data. Our models have two ultimate goals:

1. Accurately predict the value of the housing by the data given

2. Find out the important factors that will affect the price of the house. 


Before we start to build the models, we first define two functions Rsq and rmse that will calculate the R^2 and RMSE. These two functions will be used to test the model accuracy. 

And After that, we construct a new dataset county_data_modelling, which is our dataset for the modelling. We rescale the mean and the standard deviation for the variables (needed for some machine learning models) and we randomly split the dataset into two parts: train set and the test set. 

```{r}
Rsq = function(y_pred, y){
    1 - sum((y_pred - y) ^ 2) / sum((y - mean(y)) ^ 2)
}

rmse = function(y_pred, y) {
  rmse = sqrt(mean((y_pred - y)^2))
return(rmse)}

```

```{r}
county_data_modelling = county_data %>% 
    select(- County, - State)
# rescale mean 0 sd 1
rescale_mean = apply(county_data_modelling, 2, mean)
rescale_sd = apply(county_data_modelling, 2, sd)
county_data_modelling = county_data_modelling %>%
    sweep(2, rescale_mean, "-") %>%
    sweep(2, rescale_sd, "/")

# train test split 3:1
set.seed(0)
train_index = sample(1:dim(county_data)[1], floor(dim(county_data)[1] * 0.75))
train = county_data_modelling[train_index, ]
test = county_data_modelling[- train_index, ]

```


1. linear model

The first model we tried is the linear model. We think that the linear model may be the most appropriate model for our project because:

1. The linear model is easier to interpret

2. The linear model can find the factors that are important to the y-value

3. We dont have many observations, so training models may not be the best fit

```{r}
lm1 = lm(price ~ ., train)
summary(lm1)

# linear model with interaction
lm2 = lm(price ~ . ^ 2, train)
model1 = lm(price ~ 1, data=train)
n = dim(train)[1]

#AIC & BIC without the interaction terms:

#AIC:
step(model1,scope=list(lower=model1,upper=lm1),direction="both",k=2)

Aic_without_inter = lm(formula = price ~ Income_PerCapita + Regular + pct_pub_transport_log + 
    EstimatedSpecialRate + BURGLRY + ROBBERY + LARCENY + Median_Family_Income + 
    Premium + MidGrade + IDNO + pct_pers_lt_pov + pct_unemp + 
    num_shoppingmall + college_num + MURDER + EstimatedCountyRate + 
    Median_Household_Income + public_school_count + CPOPCRIM + 
    ryprop + private_school_count, data = train)

print(Rsq(predict(Aic_without_inter, train, type = "response"), train$price))
print(Rsq(predict(Aic_without_inter, test, type = "response"), test$price))
rmse(predict(Aic_without_inter, train, type = "response"), train$price)
rmse(predict(Aic_without_inter, test, type = "response"), test$price)

#BIC 
step(model1,scope=list(lower=model1,upper=lm1),direction="both",k=log(n))

Bic_without_inter = lm(price ~ Income_PerCapita + pct_pub_transport_log + 
    EstimatedSpecialRate + BURGLRY + ROBBERY + LARCENY + Median_Family_Income + 
    Premium + MidGrade + IDNO, data = train)

print(Rsq(predict(Bic_without_inter, train, type = "response"), train$price))
print(Rsq(predict(Bic_without_inter, test, type = "response"), test$price))
rmse(predict(Bic_without_inter, train, type = "response"), train$price)
rmse(predict(Bic_without_inter, test, type = "response"), test$price)

#BIC with intercation:
step(lm1,scope=list(lower=model1,upper=lm2),direction="both",k=log(n))

Bic_with_inter = lm(price ~ Income_PerCapita + Median_Family_Income + Population + 
    Num_Household + hilton_count + IDNO + CPOPCRIM + RAPE + ROBBERY + 
    BURGLRY + LARCENY + Regular + MidGrade + Premium + Diesel + 
    college_num + hwyprop + ryprop + pct_pub_transport_log + 
    pct_unemp + private_school_count + Income_PerCapita:Diesel + 
    hilton_count:hwyprop + IDNO:Premium + hilton_count:pct_unemp + 
    Income_PerCapita:ryprop + Income_PerCapita:pct_pub_transport_log + 
    IDNO:LARCENY + RAPE:MidGrade + LARCENY:MidGrade + Diesel:ryprop + 
    hwyprop:private_school_count + IDNO:pct_unemp + Diesel:college_num + 
    hilton_count:CPOPCRIM + hwyprop:pct_pub_transport_log + Median_Family_Income:pct_pub_transport_log, data = train)
    
print(Rsq(predict(Bic_with_inter, train, type = "response"), train$price))
print(Rsq(predict(Bic_with_inter, test, type = "response"), test$price))
rmse(predict(Bic_with_inter, train, type = "response"), train$price)
rmse(predict(Bic_with_inter, test, type = "response"), test$price)

```

Base on the results above, for the linear model, we decide to use the BIC with the interactions to do the predictions because it performs the lowest RMSE on both the training set and the test set. However, it consists so many interaction terms making the model less interpretable. 

Therefore, we decide to use BIC without the interactions to explain the importance of the variables. BIC without the interactions yields slightly higher RMSE than the BIC with the interactions but includes much less variables. So that will be a great model to explain the variable importances. 

```{r}
summary(Bic_without_inter)
```

As we can see from the summaries, BIC without the intercations helps us to filter out 10 most important factors to the price of the housing:

Income_PerCapita, pct_pub_transport_log, EstimatedSpecialRate, BURGLRY, ROBBERY, LARCENY, Median_Family_Income, Premium, MidGrade, IDNO. Among these 10 most important varaibles, 4 out of 10 are correlated to the safety data. So we can conclude that the safety is the most important thing that people may consider. 2 out of 10 are correlated to the income levels. Therefore, how much money people earned also affect the price of the housing (which is pretty obvious). What makes us surprised is that 2 out of 10 are about gasoline prices (CPI data), indicating how much money people spending in their daily life also affect the housing price. Other twos are transportation data and tax rate data. 

Income per capital, BURGLRY, Premium and MidGrade really make a great impact on the price of the housing(with the large coefficients) indicating that CPI, earning, and safety are the factors that people consider the most when they are buying a house. 

What being strange here is that more robbery and larceny, higher price for the county. It may due to that because these places have higher housing price so more richer people tend to live in these areas, causing more robbery and larceny. 



If we want to learn more on other factors that may affect the price of the housing, we can look at the AIC model:

```{r}
summary(Aic_without_inter)
```

There are 22 variables included in the model, besides the top10 we have introduced earlier, there are 12 new variables that also affect the price of housing but not at that much:

Regular, pct_pers_lt_pov, pct_unemp, num_shoppingmall, college_num, MURDER, EstimatedCountyRate, Median_Household_Income, public_school_count, CPOPCRIM, ryprop, private_school_count

Among these 12, 3 out of 12 are about school data (number of colleges and the high schools), indicating that school is also an important variable to the price of the housing, 2 out of 12 are economic data: unemployment rate and poverty rate, 2 out of 12 are again safety data, indicating that safety is very important to the housing price, and there's one CPI data, one traffic data, one income data, number of shopping malls, and one tax rate data.

To conclude, from our modelling result, the important factors that will affect the housing price are (in the order of importance):

1. Safety rate

2. Income level

3. CPI (spending level)

4. Traffic situation (transportation)

5. Tax rate

6. Education (colleges and high schools)

7. Economic situation (Unemployment rate and poverty rate)

8. Number of shopping malls


random forest

We decide to try two training models to see whether they outperform the result of the linear model:

```{r}
rf1 = randomForest(price ~ ., data = train, ntree = 2000)
print(Rsq(predict(rf1, train, type = "response"), train$price))
print(Rsq(predict(rf1, test, type = "response"), test$price))
rmse(predict(rf1, train, type = "response"), train$price)
rmse(predict(Aic_without_inter, test, type = "response"), test$price)
```

Although the random forest algorithm pretty good on the training set, it did not perform well on the test set (even worse than the linear model BIC with the interactions). 


fully connected neural networks + early stopping + bagging (no bootstrapping)

```{r}
f = do.call(paste, as.list(colnames(county_data_modelling))) %>% 
    str_replace_all(" ", " + ") %>%
    str_replace("\\+", "~") %>%
    as.formula()
# bagging here
nbags = 100
# early stoppping by setting threshold
nn2 = map(1:nbags, function(i){neuralnet(f, train, hidden = 10, threshold = 1.5)})
y_train_pred = nn2 %>%
    map(function(nn){compute(nn, train[, -1])$net.result}) %>%
    unlist() %>%
    matrix(ncol = nbags) %>%
    rowMeans()
y_test_pred = nn2 %>%
    map(function(nn){compute(nn, test[, -1])$net.result}) %>%
    unlist() %>%
    matrix(ncol = nbags) %>%
    rowMeans()
print(Rsq(y_train_pred, train$price))
print(Rsq(y_test_pred, test$price))
rmse(y_train_pred, train$price)
rmse(y_test_pred, test$price)
```

The neural network prediction outperforms the other models (Much lower RMSE and much higher R^2). So we decide to use the neural networks to be the final model to do the predictions. 


## Modelling Conclusion: 

As we talked earlier, our ultimate goal of the modelling is two parts:

1. Accurately predict the value of the housing by the data given

2. Find out the important factors that will affect the price of the house. 

For the part 1, we decide to use the fully connected neural networks + early stopping + bagging method to predict for the housing price. It's pretty accurate and more precise than the other models.

For the part2, we use linear model (Bic and Aic) to select out the most important factors affecting the housing price: Safety rate, Income level, CPI (spending level), Traffic situation (transportation), Tax rate, Education (colleges and high schools), Economic situation (Unemployment rate and poverty rate), Number of shopping malls. 

We will use these results for our later recommendation sections.



## Recommendation system:

1. Find Overvalued and Undervalued Counties 

We first get the prediction averaging housing price for each county under the neural networks:

```{r}
pred = nn2 %>%
    map(function(nn){compute(nn, county_data_modelling[, -1])$net.result}) %>%
    unlist() %>%
    matrix(ncol = nbags) %>%
    rowMeans()
```

Because we have do the rescaling before, so we have to transmute it back to the original:

```{r}
pred = pred * rescale_sd[1] + rescale_mean[1]
```

We merge the prediction result back to the original dataset and sepearte all counties into 7 recommendation categories base on our prediction price (value) and their real price: Highly Highly Recommend. A very nice investment, Highly Recommend. Great Deal, Recommend. Better than fair, Fair offer, Not Recommend. Worse than fair, Highly not Recommend. Bad deal and Highly Highly not Recommend. A very bad investment you may regret!. 

Although most of the counties are Fair offer, we do have find several counties that are being highly overvalued or undervalued. We can recommend these undervalued counties to our audience and do not recommend these overvalued couties to our audience. 

```{r}
county_data = county_data %>%
    mutate(
        predictions = pred,
        diff = pred-price
    )

for(i in 1:1668){
    if(county_data$diff[i] > 0.2 ){
        county_data$recommend[i] = "Highly Highly Recommend. A very nice investment"
    }
    else if(county_data$diff[i] > 0.1 & county_data$diff[i] <= 0.2){
        county_data$recommend[i] = "Highly Recommend. Great Deal."
    }
    else if(county_data$diff[i] > 0.05 & county_data$diff[i] <= 0.1){
        county_data$recommend[i] = "Recommend. Better than fair"
    }
    else if(county_data$diff[i] < -0.05 & county_data$diff[i] >= -0.1){
        county_data$recommend[i] = "Not Recommend. Worse than fair"
    }
    else if(county_data$diff[i] < -0.1 & county_data$diff[i] >= -0.2){
        county_data$recommend[i] = "Highly not Recommend. Bad deal"
    }
    else if(county_data$diff[i] < -0.2){
        county_data$recommend[i] = "Highly Highly not Recommend. A very bad investment you may regret!"
    }
    else{
        county_data$recommend[i] = "Fair offer"
    }
}
```


2. Recommend Counties to the audience by the variables they select

As we have concluded above, the 8 factors that people will consider the most when they are buying their houses are: Safety rate, Income level, CPI (spending level)
Traffic situation (transportation), Tax rate, Education (colleges and high schools)
Economic situation (Unemployment rate and poverty rate) and Number of shopping malls. 

I want to add a new variable to our recommendation system: price. Obviously, people may consider the price when they are choosing their new homes.

We first select out important variables we concluded in the linear model which can illusrate each important factors:

```{r}
county_data_recommend = county_data %>% 
    select(price, Income_PerCapita, Regular, pct_pub_transport_log,EstimatedSpecialRate, BURGLRY, ROBBERY, LARCENY, Median_Family_Income, Premium, MidGrade, pct_pers_lt_pov,pct_unemp,
num_shoppingmall, college_num, MURDER, EstimatedCountyRate ,public_school_count,CPOPCRIM, ryprop, private_school_count) 


county_data_recommend = county_data_recommend %>%
    mutate(price = 100 - ntile(price, 100),
        Income_PerCapita = ntile(Income_PerCapita, 100),
        pct_pub_transport_log = ntile(pct_pub_transport_log, 50) * 2,
        EstimatedSpecialRate = 100 - ntile(EstimatedSpecialRate, 50) * 2,
        BURGLRY = 100 - ntile(BURGLRY, 50) * 2,
        ROBBERY = 100 - ntile(ROBBERY, 50) * 2,
        LARCENY = 100 - ntile(LARCENY, 50) * 2,
        Premium = 100 - ntile(Premium, 50) * 2,
        Regular = 100 - ntile(Regular, 50) * 2,
        MidGrade = 100 - ntile(MidGrade, 50) * 2,
        pct_pers_lt_pov = 100 - ntile(pct_pers_lt_pov, 100),
        pct_unemp = 100 - ntile(pct_unemp, 100),
        num_shoppingmall = ntile(num_shoppingmall, 16) * 6,
        college_num = ntile(college_num, 10) * 10,
        MURDER = 100 - ntile(MURDER, 50) * 2,
        EstimatedCountyRate = 100 - ntile(EstimatedCountyRate, 50) * 2,
        public_school_count = ntile(public_school_count, 10) * 10,
        private_school_count = ntile(private_school_count, 10) * 10,
        CPOPCRIM = 100 - ntile(CPOPCRIM, 100),
        ryprop = ntile(ryprop,100)
        ) %>% rowwise() %>%
    mutate(safety = mean(BURGLRY,ROBBERY,LARCENY,MURDER,CPOPCRIM),
           income = Income_PerCapita,
           transportation = mean(pct_pub_transport_log,ryprop),
           cpi = mean(Premium,Regular,MidGrade),
           economic = mean(pct_pers_lt_pov,pct_unemp),
           tax = mean(EstimatedSpecialRate,EstimatedCountyRate),
           school = mean(public_school_count,private_school_count,college_num),
           shopping = num_shoppingmall,
           Cheap = price 
           )

county_data_recommend_for_shiny = county_data_recommend %>%
    select(Cheap,safety,income,transportation,cpi,economic,tax,school,shopping)

county = county_data[,1] %>% as.data.frame() 
colnames(county) = "County"
county_data_recommend_for_shiny = bind_cols(county,county_data_recommend_for_shiny)

```






