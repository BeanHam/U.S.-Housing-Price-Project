---
title: "sta523 final project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

I. Introduction 

For this project, we want to analyze the average housing prices across different counties in the United States. Nowadays, more people begin to immigrate to the United States, including some of our friends. They move to different counties acrossing the country and brought their own houses. However, the price of their houses acting so differently acrossing each counties. 

Therefore, we are curious about the factors that will affect the price of the houses and we want to analyze the housing prices with different variables trying to find out the reasons behind that phenomenom. We also want to do some data visualizations to present a more clear picture to our audience about the distributions of housing price across different states and some time series housing price plots for some major counties. 


II. Project goals and the outlines 

1. To make an acurrate model that can predict the housing price with variables given.

2. Finding out the factors that may potentially affect the price of the houses. 

3. Data visualizations on housing price distributions and housing trends.

4. Built UI dashboard (Shiny) to visualize data and deliver data insight to audiences including plots & tables


III. Selecting the variables 

We use two methods to choose the variables that we want to scrape:

1. Focus group research

We conduct a survey to reach out our friends and our parents' friends who recently brought the houses in the United States or who planned to buy the houses in the United States. We ask them the factors that they thought are most important when they are planning to buy a house. We summarized these top factors into the potential variables pool one. 

2. Research Paper 

We read the research paper written by other people to learn which factors they may think are important to the price of houses. Some of the reseach paper from the economic department are very valuable to us because these authors conducted very comprehensive surveys and complicated models to conclude the major factors driving the price of the houses. So we summarized these factors into the potential variables pool two.

Surprisingly, we discovered that some factors appear in both potential variables pool one and pool two. So we decide to use these variables for our analyze. 


IV. Scraping the data and clean the data sets.

The final variables we want to include are population data, income data, hotel data, safety data, tax rate data, gasoline price data, college data, high school data, Traffic data, shopping mall(convinience) data and Hospital data. Since the time is limited otherwise there may be some few other data that we want to include. 

We find these data sources and scrape these data down using various methods. After that we clean the dataset to make sure it's tidy and can get prepared for the final modelling. We also simplified our codes to make sure it's clear and elegant. Finally, we combined all these data into one single dataframe which will be used for the final modelling and visualizations. 


V. Data checkings 

We do the final checks before we start to build the models. We draw some scatter plots and box plots for our varaibles, checking for any anomalies or NA values. Looking for potential outliers, etc. The final preparation stage before the modelling.


VI. Modelling 


VII. Visualizations 


VIII. User Interface (Shiny)


VIIII. Housing Recommendations 


Ten.  Conclusion. 


```{r, warning = F, message = F, output = F}
library(Quandl)
library(readr)
library(dplyr)
library(rvest)
library(stringr)
library(xlsx)
library(noncensus)
library(rvest)
library(tidyverse)
library(sqldf)
library(stringi)
library(readxl)
library(tools)
library(gridExtra)
library(BAS)
library(randomForest)
library(neuralnet)
suppressMessages(library(shiny))
suppressMessages(library(shinythemes))
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
suppressMessages(library(urbnmapr))
'%ni%' <- Negate('%in%')
```

## 1. base data

The base data, which is the housing price data across different counties is the base data we use for this project. 

This dataset contains the time series data about the housing price starting from 2013 to 2017. However, we only need the median housing price for each counties in Jan 2017 for the analyze. 

Because of that, the variables we scrape later also are data presented in 2017.

```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("pricepersqft.csv") %>%
  select(County,State, `17-Jan`) %>%
  group_by(County,State) %>%
  summarize(price = median(`17-Jan`))
```


## 2. Population data from wikipedia

This dataset contains the variables: Population, Per capita income, Median Household income, Median family income, and Numberof Households across each counties. The dataset is stored in Wikipedia. 

So we use the method we have learned from the class and the previous homework (read_html) to scrape this dataset down from the wikipedia. The data is stored in the wikitable so the html_nodes is "table.wikitable.sortable" and we only want the data in the first list so we use .[[1]]

At last we clean the data by filtering all empty values and replace all . with "". We cleaned the variable names and the variable classes and finally bind that tidy population dataset with our previous base dataset.

```{r message=FALSE, warning=FALSE}
price_2017 = "https://en.wikipedia.org/wiki/List_of_United_States_counties_by_per_capita_income" %>%
  read_html() %>%
  html_nodes("table.wikitable.sortable") %>%
  {.[[1]]} %>%
  html_table() %>%
  filter(State != "") %>%
  mutate_all(funs(str_replace_all(., "[\\$,]", ""))) %>%
  transmute(County = `County-equivalent`,
            State = state.abb[match(State, state.name)],
            Income_PerCapita = as.numeric(`Per capitaincome`),
            Median_Household_Income = as.numeric(Medianhouseholdincome),
            Median_Family_Income = as.numeric(Medianfamilyincome),
            Population = as.numeric(Population),
            Num_Household = as.numeric(`Number ofhouseholds`)) %>%
    merge(price_2017,.,by = c('County','State'))
```


## 3. hilton data within the country

For this dataset, we want to get the numbers of Hilton in each counties. Actually the data we want to get is the hotel data, because we think that whether there's a large hotel inside the county can implies whether that's prosperous county. Often a prosperous county indicates high median housing price because more people are tend to live there. Therefore, we scrape the hilton data from the hilton website.

Just as we did in the homework, we first get all Urls for different hiltons acrossing the country and within these urls, we scrape the location of each hotels. However, we dont need the whole location because that's useless for us. We only need to know which counties it belongs. 

Therefore, we only need the zipcode within the location to match the hotel to the county. So we manipulate the location with several stringr functions such as str_replace, strsplit, substr to only get all ziocodes for all hiltons in the United States. 

Since We avoid the for loop we largely reduce the running time of scraping that data. But because we need to scrape so many websites within that single chunck so it may cause some time to run this chunk (but much shorter than we spent the time in our Denny's homework).  

And the next step for us is to match the zipcodes to the counties. So we find a dataset online which contains all zipcodes, cities, counties, states data in the United States. This dataset is very important to us and we use a lot in our next few chuncks because it not only can match the zipcodes to the county but can also match the cities to counties.

So after we match the zipcodes with the corrsponding counties, there may be some counties which have more than 1 hilton in it. Therefore, we need to group by the counties and summarize the sum of hilton numbers within that counties. 

The final hilton data we merge into the base dataset is the number of hiltons within each counties. And we replace all NA with 0s. 

```{r message=FALSE, warning=FALSE}
zip_county_lookup = read_csv("ZIP-COUNTY-FIPS_2018-03.csv") %>%
    mutate(County = COUNTYNAME)

urls = read_html("https://www3.hilton.com/en/hotel-locations/us/index.html") %>%
    html_nodes(.,'ul.directory_locations_list li a') %>% 
    html_attr(., "href") %>% paste0("https://www3.hilton.com", .)

price_2017 = sapply(urls, function(url){
     url %>%
         read_html() %>%
         html_nodes('ul.directory_locations_list li a') %>%
         {paste0("https://www3.hilton.com", html_attr(., "href"))} %>%
         sapply(function(locationURL){
             locationURL %>%
                 read_html() %>%
                 html_nodes('ul.directory_hotels_list li') %>%
                 html_text() %>%
                 str_replace("\t", "") %>%
                 str_replace("\r\n", "") %>%
                 {strsplit(., " ")[[1]]} %>%
                 {.[length(.)]} %>%
                 {strsplit(., "USA")[[1]][1]} %>%
                 substr(1, 5)
             })
 }) %>%
     unlist() %>%
     unname() %>%
    as.data.frame() %>%
    {colnames(.)[1] = "zips"; .} %>%
    merge(.,zip_county_lookup,by.x ='zips',by.y = 'ZIP') %>%
     mutate(COUNTYNAME = COUNTYNAME %>%
                str_locate_all(" ") %>%
                sapply(function(mat){mat[min(dim(mat), 2), 1] - 1}) %>%
                {str_sub(COUNTYNAME, 1, .)}) %>%
  group_by(COUNTYNAME,STATE) %>%
  summarize(count = length(COUNTYNAME)) %>%
    mutate(hilton_count = count) %>% 
    dplyr::select(COUNTYNAME,STATE,hilton_count) %>%
    left_join(price_2017, ., by = c("County"="COUNTYNAME","State" = "STATE")) %>%
  mutate_all(funs(replace(., is.na(.), 0)))
```


## 4. clean the dataset 

In order to merge more easily and efficiently for the later chuncks, we want to modify the County column in both base datasets and the zip_county_lookup(the matching dataset.)

For both datasets, we remove County, Borough, Parish, Munipality, Municipio, Island and We replace all "city" by "City". Now the two datasets' County column are identical and easy to merge in the later tasks. 

```{r message=FALSE, warning=FALSE}
clean_county = function(df){
  df %>%
    mutate(County = str_replace(County, " County,*$| Borough,*$| Parish,*$| Municipality,*$| Municipio,*$| Island,*$", "") %>%
             str_replace(" [cC]ity,*$", " City"))
}

price_2017 = clean_county(price_2017) 
zip_county_lookup = clean_county(zip_county_lookup) %>%
  select(ZIP,STATE,County,CITY)
```

## 5. crime rate data

For this chunck we want to get the crime rate data for each county. Cause safety is one of the most important factor to consider when people are buying houses to live. No one wants to live in a very dangerous place. 

Therefore, we find out the crime dataset from the online sources and we acquire the safety related variables such as crime percent, murder rate, rate rate, robbery rate, theft rate etc. 

We clean the crime dataframe and merge it into the base dataset. Since in the crimerate dataset, the county and state are stick together: aaa county, xx state. Therefore, we use strsplit and mutate to divide the county and state and we use the clean_county function we defined above to clean the crime_data dataframe.After that, we merge it with the base dataset.

However, one observation missing after the merging and after checking for that variables we discover that in crime dataset that obversvation names "Larue" but it should named "LaRue" in our base dataset. So we modified that observation and merge agian. 

```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("crime_data_w_population_and_crime_rate.csv") %>%
  select(county_name, crime_rate_per_100000, IDNO, CPOPCRIM, CPOPARST, COVIND,
         MURDER, RAPE, ROBBERY, AGASSLT, BURGLRY, LARCENY, MVTHEFT, ARSON) %>%
  mutate(county_name = strsplit(county_name, " ")) %>%
  rename(County = county_name) %>%
  mutate(State = County %>% sapply(last),
         County = County %>% sapply(head, -1) %>% 
           sapply(function(v) {do.call(paste, as.list(v))})) %>%
  clean_county() %>%
    left_join(price_2017,.,by=c('County','State'))
```

## 6. tax rate data

We decide that the tax rate is also a very important indicator to the housing price because people may want to live in the places with lower tax rates. So we get the online data sources for the tax rates for all zips in the United States.

However, that data is a zipfile and after unzip it, it becomes a folder. So we first read all csv data into one single dataframe and we join it with the counties by using the zip_county_lookup. We choose the appropriate varialbes that can illustrate the tax rate and use {.[!duplicated(.[,1:2]), ]} as to filter out only unique Counties. 

Finally, we merge that tax rate dataset with our base dataset. 

```{r message=FALSE, warning=FALSE}
setwd("TAXRATES_ZIP5")

price_2017 = list.files(pattern = "*.csv") %>%
  map(read.csv) %>%
  {do.call(bind_rows, .)} %>%
  left_join(zip_county_lookup,by=c("ZipCode"="ZIP")) %>%
  select(County, State, StateRate, EstimatedCombinedRate, EstimatedCountyRate,
         EstimatedCityRate, EstimatedSpecialRate, RiskLevel) %>%
  {.[!duplicated(.[,1:2]), ]} %>%
    left_join(price_2017,.,by=c('County','State')) 
```

## 7. gasoline price data

We think that gasoline price can be a very interesting indicator that can show the price of the housing. This was the result illustrated by one of the paper we read. Gasoline price can somehow reflect an area's CPI and thus making an impact on the housing price.

So we find a website containing the gasoline price for each states in the United States (We tried very hard but we cannot find the average gasoline price by County unless we have to pay 50+ dollar for it). We write the function get_gasoline to scrape the data from this website and to directly construct a dataframe by using the data scraped by this function.

However, in the website the price is stored as for example: $3.566. This is a factor level. Therefore, we use the gsub to remove all dollar signs and change all the classes to the numeric. 

Finally, we merge the gasoline dataset with our base dataset. 

```{r message=FALSE, warning=FALSE}
get_gasoline = function(k){
    web = read_html("https://gasprices.aaa.com/state-gas-price-averages/")
    dat = trimws(html_text(html_nodes(web, paste0("td:nth-child(",k,")"))))
    }

price_2017 = data.frame(State=get_gasoline(1), Regular=get_gasoline(2), MidGrade=get_gasoline(3), Premium=get_gasoline(4), Diesel=get_gasoline(5)) %>% lapply(., gsub, pattern="\\$", replacement="") %>% 
    as.data.frame() %>% 
    mutate(Regular = as.numeric(as.character(Regular)),
           MidGrade = as.numeric(as.character(MidGrade)),
           Premium = as.numeric(as.character(Premium)),
           Diesel = as.numeric(as.character(Diesel))) %>%
    mutate(State = state.abb[match(State,state.name)]) %>%
    left_join(price_2017,.,by='State')
```

## 8. number of colleges

We think that the number of college is also a very important indicator to the price of the houses because college will attract many students coming from different places in the world to that county and these students need place to live. 

So we scrape the data from the IPEDS Data Center to get all universities/colleges in the country and their corresponding cities. Then we use the data zip_county_lookup again to get the reference from cities to counties. We dont need the first column (school IDS) and we dont need the first row (title and columnnames). 

After matching these schools with their corresponding counties we summarize the number of colleges in each counties and finally we merge that college dataset to our base dataset. We replace NA with 0s. 

```{r message=FALSE, warning=FALSE}
## reference: https://nces.ed.gov/ipeds/datacenter/InstitutionProfile.aspx

cities = zip_county_lookup %>% select(CITY, County, STATE) %>% unique() %>%
    rename(State = STATE, City = CITY)

price_2017 = read_html("IPEDS Data Center.html") %>%
    html_node("table.idc_gridview") %>%
    html_table() %>%
    select(-X1) %>% .[-1,] %>%
    rename(school = X2, City = X3, State = X4) %>%
    left_join(.,cities, by=c('City','State'))%>% 
    group_by(County, State) %>%
    summarise(college_num = n()) %>%
    left_join(price_2017, ., by=c('County','State')) %>%
    mutate(college_num = ifelse(is.na(college_num), 0, college_num))
```


## 9. Traffic data 

Both the paper and the research group shows that the Traffic situation really affect the housing prices. No ones want to live in a place where they have to spent hours trafficing on the road. So we find a traffic dataset from the online sorces and after some manipulations on the dataset, we merge this data to our base dataset(dont talk too much here because it's very similar to what we did above).


```{r message=FALSE, warning=FALSE}
price_2017 = read_csv("EQIDATA_ALL_DOMAINS_2014MARCH11.CSV") %>% 
  select(county_name,state,hwyprop,ryprop,
         pct_pub_transport_log,fatal_rate_log,
         pct_pers_lt_pov,pct_unemp) %>% 
  rename(County=county_name,State=state)%>% 
  mutate(County=str_replace_all(County,"County","") %>% 
           str_trim(side = "both")) %>%
    left_join(price_2017, .,by=c('County','State'))%>%
  mutate_all(funs(replace(., is.na(.), 0)))
```

## 10. shopping mall data 

Number of Shopping mall is an important indicator to the housing price because of 

1: if a county has one or more shopping mall it can at least show it's a prosperous county

2: Shopping mall itself can attract people come/ travel to that county

3. More shopping mall implies more people live nearby, thus higher housing price.

Therefore, we scrape the shopping mall data from the online sources.


However, that one is a bit tricky. Because for different states, shopping mall are stored in different websites, and the storing patterns are not the same. Some are stored in the table form, some are stored in the list form. And for both table form and list form, different webpage has different stroing methods. So basically we are scraping many different websites in this part and these websites are not identical.

For data stored in wikitable forms, it has two categories: needed to be flatten and dont needed to be flatten. So I write two different functions o scrape these data. 

For the table which needs to be flatten:

the input are state name (used for the url), n1, the second data list we needed (the first data list is 1), k, the interval between each data list we needed, and n4,n5,l4,l5 are just the last two names and last two locations of shopping mall data list we needed. We need these n4,n5,l4,l5 because for some states they only have 4 data lists we needed for both name and location, so we can use NA for these 4 inputs. 

After we read and flatten the wikitable, we got a list that inside this large list, some of the list contains the name and location we want and some contains the information we dont want. So we have to acquire the corresponding data we want and to make it into the dataframe with two columns: name and location. And we return that dataframe. 


For the table dont need to be flatten:

That's much easier. we simply read that wikitable and the whole list did not contain the data we dont want. Simply all names are stored in the list 1 and all locations are stored in the list 2. So we return that dataset with two columns: name and location. The input is only state using for the url.


For the lists:

We use "html_nodes(page,"li") %>% html_text()" to scrape the corresponding data we needed from the list in wikipedia. 

We also need an input n because in the lists, after the shopping mall name and location, there are also other information stored in that list that we dont want. Therefore, because we already know the number of shopping mall within each state, we have to input that n (the number of shopping mall in that state = the number of useful rows in the list) so we are not getting any annoying data. 

This is not hardcoding because there's no other way to do it more efficiently and we have tried our best to simplify the code here.


Therefore, we scrape the data for different states directly using these three functions but for texas, it's different. Because in the texas shopping mall website, data is stored in different wikitables and lists. So i write a function read_taxas and the input p is simply the ID of wikitable stored in that website.


After scraping all these dataset down we have to clean the location data because the original dataset are very messy. We use seperate the select to remove all unuseful (), latitute, longitude, state information inside the location.

For each state's data We scrape, we mutate a new column state and generate the corresponding state to it in order for the group by below.

Finally, we bind all these data into the dataframe shopping mall data. 

In that final shopping mall data we still have some problems: all letters are in the uppercase and there's some head blanks in some of the observations.

So we did some data manipulations and merge it with the cities data we constructed before (to match the city to the counties).

After matching to the counties, we summarize the number of shopping malls in each counties and merge it with the base dataset. Replacing NA with 0s. 

```{r message=FALSE, warning=FALSE}
read_wikitable_flat = function(state,n1,k,n4,n5,l4,l5){
    l1 = n1+1
    url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
    dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  html_table() %>% flatten()
  data=data.frame(
  name=c(dat[[1]] %>% as.character(.),dat[[n1]] %>% as.character(.),
         dat[[n1+k]] %>% as.character(.),dat[[n1+2*k]] %>% as.character(.),
         dat[[n4]] %>% as.character(.),dat[[n5]] %>% as.character(.)
  ),
  location=c(dat[[2]] %>% as.character(.),dat[[l1]] %>% as.character(.),
             dat[[l1+k]] %>% as.character(.),dat[[l1+2*k]] %>% as.character(.),
             dat[[l4]] %>% as.character(.),dat[[l5]] %>% as.character(.)
  ))
  return(data)
}

read_wikitable_nonflat = function(state){
  url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
  dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  html_table() 
  data = data_frame(
  name = dat[[1]][[1]] %>% as.character(.),
  location=dat[[1]][[2]] %>% as.character(.)
)
  return(data)
}

read_wikilist = function(state,n){
url = paste0("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_",state)
page = read_html(url)
dat = data_frame(
  name=html_nodes(page,"li") %>% html_text() 
) %>% .[1:n,] %>%
  separate(name, c("name", "location"), "-",remove=TRUE)
}

read_texas = function(p){
 url = "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Texas"
 dat = read_html(url) %>%
  html_nodes("table.wikitable.sortable") %>%
  .[[p]] %>% 
  html_table(fill=TRUE) 
dat = dat[,1:2]
colnames(dat)<-c("name","location")
return(dat)
}

shooping_mall_num = read_xlsx("shooping_mall_num.xlsx",1)
shopping = shooping_mall_num[rep(row.names(shooping_mall_num), shooping_mall_num$'Number of Malls'), 1] %>%
  as.data.frame() %>% .[1:821,]

cities$City = tolower(cities$City)

page = read_html("https://en.wikipedia.org/wiki/List_of_shopping_malls_in_the_United_States")

price_2017 = bind_rows(
    al_data = read_wikitable_nonflat("Alabama") %>% mutate('STATE' = 'AL'),
    ca_data = read_wikilist("California",181) %>%
  separate(location, c("location", "state"), ",",remove=TRUE, extra = "merge") %>% 
  select(name, location) %>% 
  mutate('STATE' = 'CA'),
  md_data = read_wikilist("Maryland",34) %>% mutate('STATE' = 'MD'),
  mi_data = read_wikitable_flat("Michigan",8,7,29,36,30,37) %>% 
    separate(location, c("location", "latitude"), "4",remove=TRUE, extra = "merge") %>% select(name, location) %>% 
  mutate('STATE' = 'MI'),
  nj_data = read_wikitable_nonflat("New_Jersey") %>% mutate('STATE' = 'NJ'),
  or_data = read_wikitable_flat("Oregon",7,5,NA,NA,NA,NA) %>% 
  mutate('STATE' = 'OR'),
  pa_data = read_wikitable_nonflat("Pennsylvania") %>% mutate('STATE' = 'PA'),
  tx_data = rbind(read_texas(1),read_texas(2),read_texas(3),
                texas = read_wikilist("Texas",50) %>% .[15:50,] %>% separate(location, c("location", "other"), "[(]",remove=TRUE, extra = "merge") %>% 
  select(name, location)) %>% 
  mutate('STATE' = 'TX'),
  st_data = data_frame(
  name=html_nodes(page,"li") %>% html_text() 
) %>% .[59:879,] %>% 
  separate(name, c("name", "location1"), " . ",remove=TRUE)%>%
  separate(name, c("name", "location2"), "–",remove=TRUE, extra = "merge") %>% 
  mutate(location = coalesce(location1,location2)) %>%
  select(name, location) %>% mutate(STATE = shopping)) %>%
  separate(name, c("name", "other"), "[(]",remove=TRUE) %>%
  select(name, location, STATE)%>%
  separate(location, c("location", "other"), "[(]",remove=TRUE) %>%
  select(name, location, STATE) %>%
    rename(City = location, State = STATE) %>%
    mutate(City = tolower(City) %>% str_trim(.,"left")) %>%
  merge(.,cities,by=c("City","State")) %>%
  select(name,City,State,County) %>% unique() %>%
  group_by(County,State) %>%
  summarise(num_shoppingmall = length(County)) %>%
  left_join(price_2017,.,by=c("County","State")) %>%
    mutate(num_shoppingmall = ifelse(is.na(num_shoppingmall), 0, num_shoppingmall))
    
    

```

## 11. Public high school data

As the same reason as the colleges, high school also attract people coming from different places around the world. So we find the high school data (both public and private) from the online data sources and merge it into our base dataset.

We dont want to illustrate too much here because basically we are using the very similar data cleaning as we are using above. 

```{r message=FALSE, warning=FALSE}
price_2017 = read.csv("Public_high_school.csv", stringsAsFactors=FALSE) %>% 
    select(1, 2, 4) %>%
    rename(State = County.Name, County = County.Name..Public.School..2015.16, public_school_num = Total.Number.of.Public.Schools..Public.School..2015.16) %>%
    mutate(County = str_trim(str_replace(.$County, "County", "")),
           public_school_num = as.numeric(public_school_num),
           State = stri_sub(str_trim(State), -2, -1)) %>% 
    na.omit() %>%
    group_by(County, State) %>% 
    summarise(public_school_count = sum(public_school_num, na.rm = TRUE)) %>%
    left_join(price_2017, ., by=c('County','State')) %>%
    mutate(public_school_count = ifelse(is.na(public_school_count), 0, public_school_count))
```

## 12. Private high school data

The same reason as in the part 11.

For here we use state.abb[match(toTitleCase(tolower(State)),state.name)] to transform the state name in the abbreviations by using the function state.abb. toTitleCase is to only transform the first letter into capital so it can match to the state.name. 

```{r message=FALSE, warning=FALSE}
price_2017 = read.csv("Private_high_school.csv", stringsAsFactors=FALSE) %>% 
    select(2, 5) %>% 
    rename(State = State.Name..Private.School..Latest.available.year, County = County.Name..Private.School..2015.16) %>% 
    mutate(State = state.abb[match(toTitleCase(tolower(State)),state.name)],
           County = toTitleCase(tolower(County))) %>%
    group_by(County, State) %>% 
    summarise(private_school_count = n()) %>%
    left_join(price_2017, ., by=c('County','State')) %>%
    mutate(private_school_count = ifelse(is.na(private_school_count), 0, private_school_count))

```


## 13. Hospital data

Hospital is a very important indicator for the housing price because everyone will sick in some of their life and they want good hospitals to treat.Therefore, if a county has many good hospitals it can attract people to live here.

Therefore, we find a dataset containing all hospitals in the country and also their ratings, emergency service, etc. 

We summarize number of hospitals, number of emergency hospitals and average hospital ratings for each country and merge them to the base dataset. Replacing NA with 0 and if a county doesnt have a hospital will replace 0 with its rating. 


```{r message=FALSE, warning=FALSE}
price_2017 = read.csv("HospInfo.csv", stringsAsFactors=FALSE) %>%
    select(Hospital.Name, County.Name, State,Emergency.Services, Hospital.overall.rating ) %>% rename(Name = Hospital.Name, County = County.Name, Emergency= Emergency.Services, rating = Hospital.overall.rating) %>%
    mutate(County = toTitleCase(tolower(County)),
           Emergency = as.numeric(Emergency),
           rating = as.numeric(rating)) %>%
    group_by(County, State) %>%
    summarise(num_hospitals = length(County),
              num_emergency = sum(Emergency),
              hospital_rating = mean(rating,na.rm = TRUE)) %>%
    mutate(hospital_rating = ifelse(is.nan(hospital_rating), 0, hospital_rating)) %>% left_join(price_2017, ., by=c('County','State')) %>%
    mutate(num_hospitals = ifelse(is.na(num_hospitals), 0, num_hospitals),
           num_emergency = ifelse(is.na(num_emergency), 0, num_emergency),
           hospital_rating = ifelse(is.na(hospital_rating), 0, hospital_rating))
```


```{r}
county_data = price_2017
save(county_data, file = "county_data.RData")
```


Model EDA

```{r}
create_plot = function(input, name){
    if(is.factor(input)){
        p = data_frame(
            price = county_data$price,
            input = input
        ) %>% 
            ggplot(aes(x = input, y = price)) + 
            geom_boxplot() + 
            labs(x = name)
    }else{
        p = data_frame(
            price = county_data$price,
            input = input
        ) %>% 
            ggplot(aes(x = input, y = price)) + 
            geom_point() + 
            labs(x = name)
    }
    return(p)
}

subplots = function(df, cols){
    eda_plots = map(cols, function(i){create_plot(df[, i], names(df)[i])})
    grid.arrange(grobs = eda_plots, nrow = 3, ncol = 3)
}

df = county_data %>% 
    select(- County, - State, - price) %>%
    mutate(hilton_count = as.factor(hilton_count))
### change df's certain column into factor before inputting into `subplots` 
### if boxplot is preferred for this column's predictor

walk(1:(ncol(df) %/% 9 + 1), function(group_num){
    col_index = (group_num * 9  - 8):min(group_num * 9, ncol(df))
    subplots(df, col_index)
})
```


Modelling

```{r}
county_data = county_data %>% 
    select(- County, - State)
# rescale mean 0 sd 1
rescale_mean = apply(county_data, 2, mean)
rescale_sd = apply(county_data, 2, sd)
county_data = county_data %>%
    sweep(2, rescale_mean, "-") %>%
    sweep(2, rescale_sd, "/")
# train test split 3:1
set.seed(0)
train_index = sample(1:dim(county_data)[1], floor(dim(county_data)[1] * 0.75))
train = county_data[train_index, ]
test = county_data[- train_index, ]
# R-square function
Rsq = function(y_pred, y){
    1 - sum((y_pred - y) ^ 2) / sum((y - mean(y)) ^ 2)
}
```

linear model

```{r}
lm1 = lm(price ~ ., train)
print(Rsq(predict(lm1, train, type = "response"), train$price))
print(Rsq(predict(lm1, test, type = "response"), test$price))
# linear model with interaction
lm2 = lm(price ~ . ^ 2, train)
print(Rsq(predict(lm2, train, type = "response"), train$price))
print(Rsq(predict(lm2, test, type = "response"), test$price))
### AIC/BIC selection could be used here ###
```


Bayesian Model Averaging

```{r}
bma1 = bas.lm(price ~ ., train)
# predictors with posterior probability large than 0.1:
summary(bma1)[, 1] %>%
    .[. > 0.1 & !is.na(.)] %>%
    names() %>%
    print()
# select useful variables from BMA
cols = summary(bma1)[, 1] %>%
    .[. > 0.1 & !is.na(.)] %>%
    names() %>%
    {c("price", .[-1])}
train2 = train %>% select(cols)
test2 = test %>% select(cols)
# linear model again
lm3 = lm(price ~ ., train2)
print(Rsq(predict(lm3, train2, type = "response"), train2$price))
print(Rsq(predict(lm3, test2, type = "response"), test2$price))
# with interactions again
lm4 = lm(price ~ . ^ 2, train2)
print(Rsq(predict(lm4, train2, type = "response"), train2$price))
print(Rsq(predict(lm4, test2, type = "response"), test2$price))
### AIC/BIC selection for interactions here ###
```


random forest

```{r}
rf1 = randomForest(price ~ ., data = train2, ntree = 3000)
print(Rsq(predict(rf1, train2, type = "response"), train2$price))
print(Rsq(predict(rf1, test2, type = "response"), test2$price))
```


fully connected neural networks + early stopping + bagging (no bootstrapping)

```{r}
f = do.call(paste, as.list(cols)) %>% 
    str_replace_all(" ", " + ") %>%
    str_replace("\\+", "~") %>%
    as.formula()
# bagging here
nbags = 100
# early stoppping by setting threshold
nn2 = map(1:nbags, function(i){neuralnet(f, train2, hidden = 10, threshold = 1.5)})
y_train_pred = nn2 %>%
    map(function(nn){compute(nn, train2[, -1])$net.result}) %>%
    unlist() %>%
    matrix(ncol = nbags) %>%
    rowMeans()
y_test_pred = nn2 %>%
    map(function(nn){compute(nn, test2[, -1])$net.result}) %>%
    unlist() %>%
    matrix(ncol = nbags) %>%
    rowMeans()
print(Rsq(y_train_pred, train2$price))
print(Rsq(y_test_pred, test2$price))
### neural net + early stopping + bagging is slightly better than random forest
# parameters for neural nets are yet to be tunned, if time allows

```



Shiny

```{r}
county_plot=urbnmapr::counties %>% 
    rename(State=state_abbv,County=county_name) %>% 
    mutate(County=str_replace_all(County,"County","") %>% 
           str_trim(side = "both"))
state_list = unique(county_data$State)


county_data = county_data %>% 
  rename('Housing Price' = price,
         'Income Per Capita' = Income_PerCapita,
         'Crime Rate' = crime_rate_per_100000,
         'Unemployment Rate' = pct_unemp,
         'Num Of Colleges' = college_num,
         'Num Of Public Schools' = public_school_count,
         'Num Of Private Schools' = private_school_count,
         'Hospital Rating' = hospital_rating,
         'Num Of Shoppingmalls' = num_shoppingmall,
         'Unemployment Rate' = pct_unemp,
         'Risk Level' = RiskLevel)

variable_list = c("State",
                  "County",
                  "Population",
                  "Crime Rate",
                  "Risk Level",                  
                  "Housing Price",
                  "Hospital Rating",                  
                  "Num Of Colleges",
                  "Income PerCapita",  
                  "Unemployment Rate",                    
                  "Num Of Public Schools",
                  "Num Of Private Schools",
                  "Num Of Shoppingmalls")
```

```{r}

shinyApp(
  ui = tagList(
    navbarPage(
    theme = shinytheme("cosmo"),      
    "U.S. County Housing Price",
    tabPanel("County Outlook", 
             sidebarPanel(
               selectInput("state", "Select A State", choices = state_list, selected = 1),
               checkboxGroupInput("variable","Select variables to view", variable_list),
               actionButton("update", "Update"),
               hr(),
               h3("Variable Dictionary: "),
               helpText("- Crime Rate: number of crimes per 100,000"),
               helpText("- Risk Level: 9 - Highest; 0 - lowest"),
               helpText("- Housing Price: $1000/square-meters"),
               helpText("- Hospital Rating: 5 - highest; 0 - lowest"),
               helpText("- Unemployment Rate: in percentage")
               ),
             mainPanel(
               DT::dataTableOutput("table1")
               )),
    
    tabPanel("County Housing Price",
             sidebarPanel(
               selectInput("state_hist", "Select A State", choices = state_list, selected = 1),
               sliderInput("slide", 
                           label = "Choose Your Price Range", 
                           min = 0.406, max = 4, value = 2),
               actionButton("histogram", "Get Housing Distribution"),
               hr(),
               h3("Variable Dictionary: "),
               helpText("- Housing Price: $1000/m^2")
               ),
             mainPanel(
               plotOutput("distribution")
             )),
    
    tabPanel("Map View", 
             sidebarPanel(
               selectInput("feature", "Select A Plot Feature", 
                           choices =c("Housing Price","Income Per Capita"), selected = 1),
               actionButton("plot", "Plot"),
               hr()),
             mainPanel(
               plotOutput("plot1")
               )))
    ),
  
  server = function(input, output, session){
    
    ## panel1 
    tabledata = eventReactive(
      input$update,{
      county_data %>% filter(State == input$state) %>% select(input$variable)
    })
    
    output$table1 = DT::renderDataTable(DT::datatable({
      tabledata()
      }))
    
    ## panel 2
    plotdata = eventReactive(
      input$plot,{
      county_data  %>% select(input$feature,State,County) %>% 
              left_join(.,county_plot,by=c("State","County"))
    })
    
    output$plot1 = renderPlot({
    plotdata() %>%
    ggplot(aes(x=long, y=lat, group = group, fill = price)) +
    geom_polygon(color = NA) +
        geom_polygon(data = urbnmapr::states, mapping = aes(long, lat, group = group),
        fill = NA, color = "#ffffff") +
    coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
    theme(legend.title = element_text(),
        legend.key.width = unit(.5, "in"))+
    labs(fill = "price") 

    })
    
    # panel 3
    observeEvent(
      input$state_hist, {
        histdata = county_data %>% filter(State == input$state_hist)
        updateSliderInput(session, "slide", 
                          max = max(histdata$`Housing Price`))
      })
    
    histdata = eventReactive(
      input$histogram, {
        county_data %>% filter(State == input$state_hist,
                               `Housing Price`<= input$slide)
      }
    )
    
    output$distribution = renderPlot({
      plot = histdata()
      
      ggplot(data = plot, 
             aes(x = reorder(County, -`Housing Price`), 
                 y = `Housing Price`,
                 fill = `Housing Price`)) +
        geom_bar(stat = "identity",
                 color = "black",
                 position = position_dodge()) +
        geom_text(aes(label = `Housing Price`),
                  position = position_dodge(0.9),
                  vjust = 1.6,
                  size = 3.5,
                  color = "black") +
        xlab("County") +
        ylab("Housing Price ($1000/m^2)") +
        theme(axis.text.x = element_text(angle = 40, hjust = 1, vjust = 1)) +
        scale_fill_gradient(low="lightblue", high="pink")
      })
  }
  
)

```









